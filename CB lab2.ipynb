{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background\n",
    "========\n",
    "Question 1\n",
    "--------\n",
    "The EM algorithm is based on iterative repetition of two steps: expectation-step (E-step) and maximization-step (M-step). Its convergence is proved by showing that after each iteration the likelihood which has an upperbound increases. However, the algorithm does not directly optimize the log-likelihood $\\log p(X \\mid \\theta)$ as it is intractable. Instead it optimizes $Q(\\boldsymbol\\theta \\mid \\boldsymbol\\theta^{(t)}) = \\E_{\\mathbf{Z}} \\left[\\log p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol\\theta) \\mid \\mathbf{X}, \\boldsymbol\\theta^{(t)}\\right]$. Using the Gibbs' inequality we can show that increase of the log-likelihood is more than or equal to the increase of the $Q$ function:\n",
    "\\begin{equation}\n",
    "    \\log p(\\mathbf{X} \\mid \\boldsymbol\\theta) - \\log p(\\mathbf{X} \\mid \\boldsymbol\\theta^{(t)})\n",
    "\\ge Q(\\boldsymbol\\theta \\mid \\boldsymbol\\theta^{(t)}) - Q(\\boldsymbol\\theta^{(t)} \\mid \\boldsymbol\\theta^{(t)}).\n",
    "\\end{equation}\n",
    "Therefore increasing the $Q$ function we also increase the log-likelihood. As the log-likelihood is bounded, the EM algorithm converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "--------\n",
    "Although the EM algorithm performs well on many problems, it has serious limitations. One of those is while the result of the algorithm is guaranteed to be an optimum, it can be a local one. This local solution can be much worse than the global one and mainly depends on the initialization of the parameters $\\boldsymbol\\theta^0$. Therefore there presents some randomness in the output of the algorithm. This limitation can be partialy overcome by better initialization (for example with K-means) or with imporved versions of EM such as the stochastic EM (SEM).\n",
    "\n",
    "Another strong drawback of the EM is that it needs to know the number of possible states of $\\mathbf{Z}$ which can not be known in advance in some real-world problems such as clusterization. There are possible extensions such as BIC or AIC criterions to estimate the number of the states but which do not necessarily work well in practice.\n",
    "\n",
    "The last disadvantage that we should mention is the convergence speed. The EM is not guaranteed to converge fast so it can stuck on a \"plateau\" for some time and terminate the iterations while an optimum (even a local one) is not yet achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\DeclareMathOperator{\\E}{\\mathbb{E}}$\n",
    "\n",
    "Problem 1\n",
    "========\n",
    "Question 1\n",
    "--------\n",
    "The change point $z$ tells us the place where the parameter $\\theta$ of the distribution changes. Given the distribution for each $p(y_i|z,\\theta)$ we can deduce the following formula for the whole distribution first for the case $z = 1$:\n",
    "\\begin{equation}\n",
    "    p(y|z = 1, \\theta) = \\prod_{i=1}^n \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}.\n",
    "\\end{equation}\n",
    "Here we assume that every position in the sequence is independent from the rest. For the case of any given $z > 1$ we can write the following:\n",
    "\\begin{equation}\n",
    "    p(y|z, \\theta) = \\prod_{i=1}^{z-1} \\theta_1^{y_i} (1 - \\theta_1)^{1 - y_i} \\prod_{i=z}^n \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "--------\n",
    "If we take logarithm of the deduced expression, we get a formula where summation is used instead the product:\n",
    "\\begin{equation}\n",
    "    \\log{p(y|z, \\theta)} = \\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * \\sum_{i=1}^{z-1}{1 - y_i} + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * \\sum_{i=z}^{n}{1 - y_i} = \\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * (z - 1 + \\sum_{i=1}^{z-1}y_i) + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * (n - z + 1 + \\sum_{i=z}^{n}y_i).\n",
    "\\end{equation}\n",
    "The last equation is obtained using the fact the sum of ones and zeros equal to the number of positions in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "--------\n",
    "Using the definition of $R(z)$ from the document we can the Bayes formula and derive the following equation for $\\forall z = 2, \\ldots ,n$:\n",
    "\\begin{equation}\n",
    "    R(z) = \\frac{p(z|y,\\theta)}{p(z = 1|y,\\theta)} = \\frac{p(y|z,\\theta) * p(z|\\theta) * p(y|\\theta)}{p(y|z=1,\\theta) * p(z=1|\\theta) * p(y|\\theta)} = \\frac{p(y|z,\\theta) * \\frac{1}{n}}{p(y|z=1,\\theta) * \\frac{1}{n}} = \\frac{p(y|z,\\theta)}{p(y|z=1,\\theta)}.\n",
    "\\end{equation}\n",
    "We also used the fact that the change point is distributed uniformely. If we use the formulas from the first question, we deduce the following formula:\n",
    "\\begin{equation}\n",
    "    R(z) = \\frac{\\prod_{i=1}^{z-1} p(y_i|\\theta_1,z) \\prod_{i=z}^n p(y_i|\\theta_2,z)}{\\prod_{i=1}^n p(y_i|\\theta_2,z=1)} = \\prod_{i=1}^{z-1}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}\n",
    "\\end{equation}\n",
    "where we use the fact that $p(y_i|\\theta_2,z) = p(y_i|\\theta_2,z=1) = \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}$ for all $i \\geq z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "--------\n",
    "We also can deduce a relationship between $R(z) = \\prod_{i=1}^{z-1}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}$ and $R(z-1) = \\prod_{i=1}^{z-2}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}$ for all $z = 2, \\ldots, n$ as following:\n",
    "\\begin{equation}\n",
    "    R(z) = R(z-1) \\frac{p(y_{z-1}|\\theta_1,z)}{p(y_{z-1}|\\theta_2,z=1)} = R(z-1) (\\frac{\\theta_1}{\\theta_2})^{y_z} (\\frac{1-\\theta_1}{1-\\theta_2})^{1-y_z}\n",
    "\\end{equation}\n",
    "We also can see that:\n",
    "\\begin{equation}\n",
    "    R(1) = 1\n",
    "\\end{equation}\n",
    "At this point let us deduce an algorithm to efficiently compute $p(z|y)$. This can be done using the previously deduced formula for $R(z)$ and the fact that $z$ is assumed to be known and therefore $p(z|y) = p(z|y,\\theta)$:\n",
    "\\begin{equation}\n",
    "    p(z|y) = p(z|y,\\theta) = R(z) * p(z=1|y,\\theta)\n",
    "\\end{equation}\n",
    "Thus, $p(z|y)$ can be estimated up to a multiplicative constant which can be estimated using the fact that $p(z|y$ is probability and its sum equals to 1: $p(z=1|y,\\theta) = \\frac{1}{\\sum_{z=1}^{n} R(z)}$.\n",
    "As it can be seen, computation of all $R(z)$ can be done in $O(n)$ steps. Once computed $R(z)$ for all $z = 1, \\ldots, n$, we can deduce $p(z|y)$ for all $z = 1, \\ldots, n$ in $O(n)$ steps as well. This gives us the following algorithm:\n",
    "<img src=\"img/p1q4.png\" alt=\"Algorithm\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "--------\n",
    "Let us evalute 4 quantities which will be needed for the final algorithm. Those quantities are:\n",
    "\\begin{equation}\n",
    "    E_1 = \\E(\\sum_{i=1}^{z-1}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_2 = \\E(\\sum_{i=z}^{n}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_3 = \\E(z - 1 - \\sum_{i=1}^{z-1}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_4 = \\E(n - z + 1 - \\sum_{i=z}^{n}y_i|\\theta^0).\n",
    "\\end{equation}\n",
    "For fixed $z, \\theta^0$ we can compute the exact values of the sums:\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{z-1}y_i = (z-1)\\theta^0_1,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\sum_{i=z}^{n}y_i = (n - z + 1)\\theta^0_2.\n",
    "\\end{equation}\n",
    "Therefore to know the expected values we have to average over all possible values of $z$:\n",
    "\\begin{equation}\n",
    "    E_1 = \\E(\\sum_{i=1}^{z-1}y_i|\\theta^0) = \\theta^0_1*\\sum_{z=1}^n(z-1)p(z|y),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_2 = \\E(\\sum_{i=z}^{n}y_i|\\theta^0) = \\theta^0_2*\\sum_{z=1}^n(n-z+1)p(z|y) = \\theta^0_2*\\left(n - \\sum_{z=1}^n(z-1)p(z|y)\\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_3 = \\E(z - 1 - \\sum_{i=1}^{z-1}y_i|\\theta^0) = \\sum_{z=1}^n\\left(z-1 -\\theta^0_1*(z-1)\\right)p(z|y) = (1-\\theta^0_1) * \\sum_{z=1}^n(z-1)p(z|y),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_4 = \\E(n - z + 1 - \\sum_{i=z}^{n}y_i|\\theta^0) = \\sum_{z=1}^n\\left(n - z + 1 - (n-z+1) * \\theta^0_2 \\right)p(z|y) = (1-\\theta^0_2) * \\sum_{z=1}^n(n-z+1)p(z|y) = (1-\\theta^0_2) * \\left(n - \\sum_{z=1}^n(z-1)p(z|y)\\right).\n",
    "\\end{equation}\n",
    "Therefore to compute the exquations above, we need to compute $\\sum_{z=1}^n(z-1)p(z|y)$ which we can calculate inside the first for loop of the previous algorithm. The complexity of the whole algorithm will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\t{\\theta}$\n",
    "$\\def\\z{j}$\n",
    "$\\def\\tt{{\\theta}^0}$\n",
    "\n",
    "Question 6\n",
    "-------\n",
    "To estimate the parameter $\\theta$, the Expecation-Maximization (EM) algorithm. We find\n",
    "\\begin{align}\n",
    "\tQ(\\theta, \\theta^0 ) &= \\E\\left[\\log(p(y,z \\mid \\theta)\\mid y,\\theta^0)\\right] \\\\\n",
    "    &= \\sum_{j = 1}^n \\log\\left(p(y,z = j \\mid \\theta)\\right) * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log\\left(p(z = j \\mid \\theta\\right)\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log{\\frac{1}{n}}\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 2}^n \\left[\\left(\\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * (z - 1 + \\sum_{i=1}^{z-1}y_i) + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * (n - z + 1 + \\sum_{i=z}^{n}y_i)\\right) + \\log{\\frac{1}{n}}\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &+ \\left(\\log{\\theta_2} * \\sum_{i=1}^n y_i + \\log{(1-\\theta_2)} * (n - \\sum_{i=1}^n y_i) + \\log{\\frac{1}{n}}\\right) * p(z = 1 \\mid y,\\theta^0).\n",
    "\\end{align}\n",
    "To maximize the provided function and make one step of the EM algorithm, we find derivatives of $Q(\\theta, \\theta^0)$ with respect to $\\theta = (\\theta_1,\\theta_2)$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_1} &= \\sum_{\\z = 2}^n  \\left( \\cfrac{1}{\\t_1} \\sum_{i=1}^{\\z-1}y_i - \\cfrac{1}{1-\\t_1}(\\z -1 -\\sum_{i=1}^{\\z-1}y_i )\\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_1(1-\\t_1)} \\sum_{\\z = 2}^n  \\left( (1-\\t_1) \\sum_{i=1}^{\\z-1}y_i - \\t_1 (\\z -1 -\\sum_{i=1}^{\\z-1}y_i) \\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "   &= \\frac{1}{\\t_1(1-\\t_1)} \\left(\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt) \\sum_{i=1}^{\\z-1}y_i - \\t_1\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt)(\\z -1)\\right),\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_2} &= \\sum_{\\z = 1}^n \\left( \\cfrac{1}{\\t_2} \\sum_{i=\\z}^{n}y_i - \\cfrac{1}{1-\\t_2}(n - \\z +1 -\\sum_{i=\\z}^{n}y_i) \\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_2(1-\\t_2)} \\sum_{\\z = 1}^n  \\left( (1-\\t_2) \\sum_{i=\\z}^{n}y_i - \\t_2 (n - \\z +1 -\\sum_{i=\\z}^{n}y_i )\\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_2(1-\\t_2)} \\left( \\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt) \\sum_{i=\\z}^{n}y_i - \\t_2\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt)(n- \\z + 1) \\right).\\\\\n",
    "\\end{align}\n",
    "\n",
    "If we solve $\\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_1} = 0$ and $\\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_2} = 0$, we obtain the formulas for the EM algorithm maximising $Q(\\t,\\tt)$:\n",
    "\\begin{equation}\n",
    "\\t_1 = \\cfrac{\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt) \\sum_{i=1}^{\\z-1}y_i}{\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt)(\\z -1)},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\t_2 = \\cfrac{\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt) \\sum_{i=\\z}^{n}y_i}{\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt)(n -\\z +1)}.\n",
    "\\end{equation}\n",
    "This gives us the following algorithm where $\\epsilon$ is some predefined small constant:\n",
    "<img src=\"img/p1q6c.png\" alt=\"Algorithm\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7\n",
    "-------\n",
    "To evalute the derived algorithm, we generate sequence of length 321 with some randomly chosen values of $\\theta$:\n",
    "```python\n",
    "generation with theta = (0.910192,0.234319), z = 52\n",
    "```\n",
    "Then we run the EM algorithm to estimate $\\theta$ and $z$. We use $\\epsilon = 0.0001$ and $L_2$ norm as a convergence criteria:\n",
    "```python\n",
    "iter 0, cur theta = (0.527564,0.376617)\n",
    "iter 1, cur theta = (0.831720,0.186754)\n",
    "iter 2, cur theta = (0.880847,0.185413)\n",
    "iter 3, cur theta = (0.881614,0.185544)\n",
    "estimated z = 52, theta = (0.881620,0.185547)\n",
    "```\n",
    "<img src=\"img/p1q7b.png\" alt=\"Histogram\" style=\"width: 600px;\"/>\n",
    "As we can see, there is only one pick in the figure which has value about 0.7. This means that our algorithm does good job and does not hesitate between neighboring values of $z$. The algorithm needed only 4 iterations to converge which takes a few fractions of second. Values of the estimated $\\theta$ are not exactly true, however very close. This is the consequence of the finite size of the sequence and presenting noise. The bigger the sequence is, the better the law of big numbers works (and therefore the closer our estimation of the mathematical expectation) so the better our algorithm will work.\n",
    "\n",
    "We ran the algorithm a few more times and observed exactly the same behaviour. We do not provide histograms here as they look exactly the same as the previous one. A few runs with random $\\theta$ initializtion are provided below:\n",
    "```python\n",
    "iter 0, cur theta = (0.447207,0.625708)\n",
    "iter 1, cur theta = (0.298836,0.000157)\n",
    "iter 2, cur theta = (0.315317,0.014550)\n",
    "iter 3, cur theta = (0.319093,0.042511)\n",
    "iter 4, cur theta = (0.398426,0.102089)\n",
    "iter 5, cur theta = (0.522450,0.137815)\n",
    "iter 6, cur theta = (0.706709,0.174965)\n",
    "iter 7, cur theta = (0.873219,0.184871)\n",
    "iter 8, cur theta = (0.881541,0.185521)\n",
    "estimated z = 52, thetas = (0.881620,0.185546)\n",
    "```\n",
    "```python\n",
    "iter 0, cur theta = (0.734940,0.254173)\n",
    "iter 1, cur theta = (0.876291,0.185521)\n",
    "iter 2, cur theta = (0.881570,0.185532)\n",
    "estimated z = 52, thetas = (0.881620,0.185547)\n",
    "```\n",
    "This shows that the data is not too difficult for the method as during the whole history of observation it converged to the global maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8\n",
    "-------\n",
    "We have chosen the uniform distribution for $z$ as we have no prior knowledge about which $z$ is more probable than another. This approach increases the entropy of the distribution and is widely used in practice. However, if we have some knowledge about the real distribution of $z$, we would have to consider it while deducing the formulas. For example, while estimating $Q(\\theta, \\theta^0 )$ we can not use the deduced formulas for $p(z = j \\mid y,\\theta^0)$ via the $R(z)$ function as we assumed there that $z$ is distributed unifromely. Instead we will have to use the following:\n",
    "\\begin{align}\n",
    "    Q(\\theta, \\theta^0 ) &= \\E\\left[\\log(p(y,z \\mid \\theta)\\mid y,\\theta^0)\\right] \\\\\n",
    "    &= \\sum_{j = 1}^n \\log\\left(p(y,z = j \\mid \\theta)\\right) * p(z = j \\mid y,\\theta^0)) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log\\left(p(z = j \\mid \\theta\\right)\\right] * p(z = j \\mid y,\\theta^0)),\n",
    "\\end{align}\n",
    "where $p(z = j \\mid y,\\theta^0) = \\cfrac{ p(y \\mid z = j, \\theta^0) p(z=j \\mid \\theta^0)}{\\sum_{k=1}^n p(y \\mid z = k, \\theta^0) p(z=k \\mid \\theta^0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 1\n",
    "========\n",
    "In this challenge we have a sequence of data consisting of 321 binary items. We have to estimate all presenting changing points with some additional information. However, our model is only developed to detect one changing point in the whole sequence. If we apply it straightforwardly, we will observe only one stable changing point:\n",
    "<img src=\"img/ch1p1.png\" alt=\"Histogram\" style=\"width: 600px;\"/>\n",
    "To study if there are more changing points, we have to mofidy the approach somehow. For example, if we know number of the changing points $k$ and their approximate locations, we can separate the sequence into $k$ subsequences and apply our original method to them. To estimate the number of changing points let us look at the data. If we just plot the sequence, it will not be too helpful however it can give us the first hint:\n",
    "<img src=\"img/ch1p2.png\" alt=\"Histogram\" style=\"width: 600px;\"/>\n",
    "We can notice that frequency of 1 from position 50 up to 150 is quite high while before and after it seems to be lower. Since this approach is not too reliable, let us plot another quantity here. We will use the Gaussian smoothing which means applying the Gaussian filter to the sequence. This means that for every point $y_i$ neighbouring points are summed with weight where the weight of point $y_j$ for $y_i$ is $\\cfrac{w_{ij}}{\\sum_{k=1}^n w_{ik}}$ and $w_{ij} = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\text{exp}\\left(\\frac{-(i - j)^2}{2\\sigma^2}\\right)$. This filter is a moving avergae where priority is given to values close to the point. The parameter of the filter is $\\sigma$ which was chosen empirically to be equal to 8 after some experiments. Here we apply the Gaussian filter to the given sequence:\n",
    "<img src=\"img/ch1p5.png\" alt=\"Histogram\" style=\"width: 600px;\"/>\n",
    "We can see dramatic drop and uprise on the positions close to the mentioned before 50 and 150. This gives us a reason to think that rapid changes on the smoothed plots correspond to changes of $z$ values. Let us prove this hypothesis by looking at generated examples with known changing points. We provide plots of smoothed sequences below where changing points are marked with squares:\n",
    "<img src=\"img/ch1p6b.png\" width=\"400\"/>\n",
    "<img src=\"img/ch1p7b.png\" width=\"400\"/>\n",
    "<img src=\"img/ch1p8.png\" width=\"400\"/>\n",
    "Thus, we confirm that the give sequence has two changing points close to 50 and 150. Next we divide the sequence into tow subsequences with ranges [0, 140] and [70, 321] and run the developed algorithm. Here is the plot we get:\n",
    "<img src=\"img/ch1p9.png\" width=\"600px\"/>\n",
    "The plot proves our hypothesis that two changing points should be observed where the big changes of values are. We provide the estiamted values, confidence intervals and estimated parameters below:\n",
    "```python\n",
    "estimated z = 68, thetas = (0.199103,0.439480), (z_l, z_u) = (54, 82)\n",
    "estimated z = 189, thetas = (0.449799,0.105608), (z_l, z_u) = (184, 194)\n",
    "```\n",
    "We also provide the output in the desired format:\n",
    "```python\n",
    "number lower upper theta1 theta2 iter\n",
    "68, 54, 82, .199103, .439480, 8\n",
    "189, 184, 194, .449799, .105608, 3\n",
    "```\n",
    "We can notice that the second changing point was detected with our model before when we assumed only one change of  $\\theta$. We provide the plots of estimated $p(z \\mid sub\\_y, \\theta)$ for both subsequences:\n",
    "<img src=\"img/ch1p10.png\" width=\"600px\"/>\n",
    "<img src=\"img/ch1p11b.png\" width=\"600px\"/>\n",
    "Here we can see that the second changing point is much more stable while the probability that the first changing point is actually located close to 55 quite high.\n",
    "\n",
    "Nevertheless we conclude that our method works reasonably well and that it is able to stably find change points. However, there are some limitations of the EM algorithm itself. For example, we have to know in advance how many change points we have. We should also mention that the second limitation of local optimality of the solution almost did not affect results of this challenge as we ran the algorithm many times and observed the same output most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Problem 3\n",
    "========\n",
    "Question 1\n",
    "--------\n",
    "We will consider each individual to be represented by a random variable:\n",
    "\\begin{equation}\n",
    "    X = (x^1,x^2),\n",
    "\\end{equation}\n",
    "where $x^1$ is the first allele of the individual and therefore can get values A,B or O. Each presons's $X$ is assumed to be i.i.d. of the other individuals'. We will note the types given in the file as following:\n",
    "\\begin{align}  \n",
    "    A &= \\{X \\mid X \\in \\{(A,A),(A,O)\\}\\},\\\\\n",
    "    B &= \\{X \\mid X \\in \\{(B,B),(B,O)\\}\\},\\\\\n",
    "    AB &= \\{X \\mid X = (A,B)\\},\\\\\n",
    "    O &= \\{X \\mid X = (O,O) \\},\n",
    "\\end{align}\n",
    "where we suppose that A/O and O/A are a same genotype and the same for B.\n",
    "Now we want to express $n_{A/A}$ as a function of $n_A, p_A$ and $p_O$. An individual of type A can have two genotypes as described before: A/A and A/O. Assuming that $n$ is big enough we can write:\n",
    "\\begin{equation}\n",
    "    n_{A/A} = n * p(X = (A,A)) = n * p(X = (A,A) \\mid X \\in A) * p(X \\in A),\n",
    "\\end{equation}\n",
    "where $n * p(X \\in A) = n_A$ and the second probability can be exrepessed using the Bayes law: $p(X = (A,A) \\mid X \\in A) = \\cfrac{p(X = (A,A), X \\in A)}{p(X \\in A)} = \\cfrac{p(X = (A,A))}{p(X \\in A)}.$\n",
    "To get the last equation we used the fact that $\\{X = (A,A)\\} \\subset \\{X \\in A\\}$. Applying the Hardy-Weinberg principle we get $\\cfrac{p(X = (A,A))}{p(X \\in A)} = \\cfrac{p_A^2}{p_A^2+2p_Ap_O}$ which gives us the final expression for $n_{A/A}$:\n",
    "\\begin{equation*}\n",
    "    n_{A/A} = n_A\\frac{p_A^2}{p_A^2+2p_Ap_O}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question 2\n",
    "--------\n",
    "The same logic will lead us to the formula for $n_{A/O}$. We will use the Hardy-Weinberg principle and obtain $\\cfrac{p(X = (A,O))}{p(X \\in A)} = \\cfrac{2p_Ap_O}{p_A^2+2p_Ap_O}$. This will give us the final result:\n",
    "\\begin{equation*}\n",
    "    n_{A/O} = n_A\\frac{2p_Ap_O}{p_A^2+2p_Ap_O}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "--------\n",
    "In this question we want to estimate $p_A$ which is the frequency of the allele A. The allele A can be found in genotypes A\\A, A\\O and A\\B while  A\\A contains the allele A two times. Therefore we can write the expression:\n",
    "\\begin{equation}\n",
    "    p_A = 2p(X = (A,A)) + p(X= (A,O)) + p(X= (A,B))\n",
    "\\end{equation}\n",
    "If we know the values of $n_{A/A}, n_{A/O}, n_{A/B}$ and $N$ which is the size of the population, we can estimate each of the probabilities of the right side and therefore $\\hat{p}_A$:\n",
    "\\begin{equation}\n",
    "    \\hat{p}_A = \\frac{2n_{A/A} + n_{A/O} + n_{A/B}}{2N}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "--------\n",
    "Using the formulas derived in the previous sections, we can estimate $\\hat{p}_A$ as a function of $n_A, N, n_{A/B}, p_A$ and $p_O$. In our problem we can observe $n_A,n_B,n_{A/B}$ and $N$. Therefore we will use the estimate of $\\hat{p}_A^{(t)}$ to estimate $\\hat{p}_A^{(t+1)}$. $p_O$ can be roughly estimated from the formula $n_O = N * p^2_O$ as following:\n",
    "\\begin{equation}\n",
    "    p_O = \\sqrt{\\frac{n_O}{N}}.\n",
    "\\end{equation}\n",
    "We will derive a better way of calculating $p_O$ in the next section but here we will use that formula. When we glue all the formulas together we get an expression to estimate $p_A$ based on previously values of $p_A$ estimated during previous iterations:\n",
    "\\begin{equation}\n",
    "    p_A^{(t+1)} = \\frac{2 n_A\\frac{(p_A^{(t)})^2}{(p_A^{(t)})^2+2(p_A^{(t)})\\sqrt{\\frac{n_O}{N}}} + n_A\\frac{2 p_A^{(t)}\\sqrt{\\frac{n_O}{N}}}{(p_A^{(t)})^2+2p_A^{(t)}\\sqrt{\\frac{n_O}{N}}} + n_{A/B}}{2N}\n",
    "\\end{equation}\n",
    "Therefore we can derive the following iterative algorithm where $\\epsilon$ is a small value defining the convergence criteria:\n",
    "<img src=\"img/p3q4.png\" alt=\"Algorithm\" style=\"width: 230px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "--------\n",
    "To derive the final EM algorithm we proceed in the same fashion as in the previous section but for all the quantities that we are interested in ($p_A, p_B, p_O$). We will provide here all the needed formulas and derive the final algorithm in the end. All the calculations are equivalent to those from the previous questions.\n",
    "If we know some current estimation of $p_A, p_B$ and $p_O$, we can compute $n_{A/A}, n_{A/O}, n_{B/B}$ and $n_{B/O}$:\n",
    "\\begin{align}\n",
    "\tn_{A/A} &= n_A \\frac{p_A^2}{p_A^2 + 2 p_Ap_O},\\\\\t\n",
    "\tn_{A/O} &= n_A \\frac{2 p_Ap_O}{p_A^2 + 2 p_Ap_O},\\\\\n",
    "    n_{B/B} &= n_B \\frac{p_B^2}{p_B^2 + 2 p_Bp_O},\\\\\t\n",
    "\tn_{B/O} &= n_B \\frac{2 p_Bp_O}{p_B^2 + 2 p_Bp_O}.\n",
    "\\end{align}\n",
    "This gives us the expectation step (E-step) of the EM algorithm. Based on those values we can compute $p_A, p_B$ and $p_O$:\n",
    "\\begin{align}\n",
    "\tp_A &= \\frac{2n_{A/A} + n_{A/O} + n_{A/B}}{2N}, \\\\\n",
    "    p_B &= \\frac{2n_{B/B} + n_{B/O} + n_{A/B}}{2N}, \\\\\n",
    "    p_O &= \\frac{2n_{O/O} + n_{A/O} + n_{B/O}}{2N}, \n",
    "\\end{align}\n",
    "which is the maximization step (M-step) of the EM algorithm. Now we put all together and present the final EM algorithm. We constraint the initial $p_A, p_B$ and $p_O$ to sum to 1.\n",
    "<img src=\"img/p3q5d.png\" alt=\"Algorithm\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6\n",
    "--------\n",
    "\n",
    "We implemented the EM algorithm using Python and tested it with given in the file input:\n",
    "\n",
    "```python\n",
    "# given input:\n",
    "n_A = 186\n",
    "n_B = 38\n",
    "n_AB = 13\n",
    "n_O = 284\n",
    "N = 521\n",
    "```\n",
    "We chose $\\epsilon$ to be small (0.000001) as here we deal with probabilities. After we run the algorithm one time, we get following values:\n",
    "```python\n",
    "# output:\n",
    "iter 0, cur p = (0.223512,0.680561,0.095927)\n",
    "iter 1, cur p = (0.287033,0.077393,0.635574)\n",
    "iter 2, cur p = (0.223861,0.051037,0.725102)\n",
    "iter 3, cur p = (0.214849,0.050184,0.734967)\n",
    "iter 4, cur p = (0.213742,0.050148,0.736110)\n",
    "iter 5, cur p = (0.213609,0.050146,0.736245)\n",
    "iter 6, cur p = (0.213593,0.050145,0.736262)\n",
    "iter 7, cur p = (0.213591,0.050145,0.736263)\n",
    "estimated p = (0.213591,0.050145,0.736264)\n",
    "```\n",
    "We conclude that these values make sense as we see that $n_O$ is big as well as $n_A$ while $n_B$ and $n_{AB}$ are less than 10% of the number of alleles. We ran the algorithm 100 times with different initial values and it always converged to the same minimum which we conclude to be the valid answer:\n",
    "\\begin{align}\n",
    "    p_A &= 0.213591 \\\\\n",
    "    p_B &= 0.050145 \\\\\n",
    "    p_O &= 0.736264\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
