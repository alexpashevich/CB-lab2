{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background\n",
    "========\n",
    "Question 1\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\DeclareMathOperator{\\E}{\\mathbb{E}}$\n",
    "\n",
    "Problem 1\n",
    "========\n",
    "Question 1\n",
    "--------\n",
    "The change point $z$ tells us the place where the parameter $\\theta$ of the distribution changes. Given the distribution for each $p(y_i|z,\\theta)$ we can deduce the following formula for the whole distribution first for the case $z = 1$:\n",
    "\\begin{equation}\n",
    "    p(y|z = 1, \\theta) = \\prod_{i=1}^n \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}.\n",
    "\\end{equation}\n",
    "Here we assume that every position in the sequence is independent from the rest. For the case of any given $z > 1$ we can write the following:\n",
    "\\begin{equation}\n",
    "    p(y|z, \\theta) = \\prod_{i=1}^{z-1} \\theta_1^{y_i} (1 - \\theta_1)^{1 - y_i} \\prod_{i=z}^n \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "--------\n",
    "If we take logarithm of the deduced expression, we get a formula where summation is used instead the product:\n",
    "\\begin{equation}\n",
    "    \\log{p(y|z, \\theta)} = \\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * \\sum_{i=1}^{z-1}{1 - y_i} + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * \\sum_{i=z}^{n}{1 - y_i} = \\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * (z - 1 + \\sum_{i=1}^{z-1}y_i) + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * (n - z + 1 + \\sum_{i=z}^{n}y_i).\n",
    "\\end{equation}\n",
    "The last equation is obtained using the fact the sum of ones and zeros equal to the number of positions in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "--------\n",
    "Using the definition of $R(z)$ from the document we can the Bayes formula and derive the following equation for $\\forall z = 2, \\ldots ,n$:\n",
    "\\begin{equation}\n",
    "    R(z) = \\frac{p(z|y,\\theta)}{p(z = 1|y,\\theta)} = \\frac{p(y|z,\\theta) * p(z|\\theta) * p(y|\\theta)}{p(y|z=1,\\theta) * p(z=1|\\theta) * p(y|\\theta)} = \\frac{p(y|z,\\theta) * \\frac{1}{n}}{p(y|z=1,\\theta) * \\frac{1}{n}} = \\frac{p(y|z,\\theta)}{p(y|z=1,\\theta)}.\n",
    "\\end{equation}\n",
    "We also used the fact that the change point is distributed uniformely. If we use the formulas from the first question, we deduce the following formula:\n",
    "\\begin{equation}\n",
    "    R(z) = \\frac{\\prod_{i=1}^{z-1} p(y_i|\\theta_1,z) \\prod_{i=z}^n p(y_i|\\theta_2,z)}{\\prod_{i=1}^n p(y_i|\\theta_2,z=1)} = \\prod_{i=1}^{z-1}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}\n",
    "\\end{equation}\n",
    "where we use the fact that $p(y_i|\\theta_2,z) = p(y_i|\\theta_2,z=1) = \\theta_2^{y_i} (1 - \\theta_2)^{1 - y_i}$ for all $i \\geq z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "--------\n",
    "We also can deduce a relationship between $R(z) = \\prod_{i=1}^{z-1}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}$ and $R(z-1) = \\prod_{i=1}^{z-2}\\frac{p(y_i|\\theta_1,z)}{p(y_i|\\theta_2,z=1)}$ for all $z = 2, \\ldots, n$ as following:\n",
    "\\begin{equation}\n",
    "    R(z) = R(z-1) \\frac{p(y_{z-1}|\\theta_1,z)}{p(y_{z-1}|\\theta_2,z=1)} = R(z-1) (\\frac{\\theta_1}{\\theta_2})^{y_z} (\\frac{1-\\theta_1}{1-\\theta_2})^{1-y_z}\n",
    "\\end{equation}\n",
    "We also can see that:\n",
    "\\begin{equation}\n",
    "    R(1) = 1\n",
    "\\end{equation}\n",
    "At this point let us deduce an algorithm to efficiently compute $p(z|y)$. This can be done using the previously deduced formula for $R(z)$ and the fact that $z$ is assumed to be known and therefore $p(z|y) = p(z|y,\\theta)$:\n",
    "\\begin{equation}\n",
    "    p(z|y) = p(z|y,\\theta) = R(z) * p(z=1|y,\\theta)\n",
    "\\end{equation}\n",
    "Thus, $p(z|y)$ can be estimated up to a multiplicative constant which can be estimated using the fact that $p(z|y$ is probability and its sum equals to 1: $p(z=1|y,\\theta) = \\frac{1}{\\sum_{z=1}^{n} R(z)}$.\n",
    "As it can be seen, computation of all $R(z)$ can be done in $O(n)$ steps. Once computed $R(z)$ for all $z = 1, \\ldots, n$, we can deduce $p(z|y)$ for all $z = 1, \\ldots, n$ in $O(n)$ steps as well. This gives us the following algorithm:\n",
    "<img src=\"img/p1q4.png\" alt=\"Algorithm\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "--------\n",
    "Let us evalute 4 quantities which will be needed for the final algorithm. Those quantities are:\n",
    "\\begin{equation}\n",
    "    E_1 = \\E(\\sum_{i=1}^{z-1}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_2 = \\E(\\sum_{i=z}^{n}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_3 = \\E(z - 1 - \\sum_{i=1}^{z-1}y_i|\\theta^0),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_4 = \\E(n - z + 1 - \\sum_{i=z}^{n}y_i|\\theta^0).\n",
    "\\end{equation}\n",
    "For fixed $z, \\theta^0$ we can compute the exact values of the sums:\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{z-1}y_i = (z-1)\\theta^0_1,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\sum_{i=z}^{n}y_i = (n - z + 1)\\theta^0_2.\n",
    "\\end{equation}\n",
    "Therefore to know the expected values we have to average over all possible values of $z$:\n",
    "\\begin{equation}\n",
    "    E_1 = \\E(\\sum_{i=1}^{z-1}y_i|\\theta^0) = \\theta^0_1*\\sum_{z=1}^n(z-1)p(z|y),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_2 = \\E(\\sum_{i=z}^{n}y_i|\\theta^0) = \\theta^0_2*\\sum_{z=1}^n(n-z+1)p(z|y) = \\theta^0_2*\\left(n - \\sum_{z=1}^n(z-1)p(z|y)\\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_3 = \\E(z - 1 - \\sum_{i=1}^{z-1}y_i|\\theta^0) = \\sum_{z=1}^n\\left(z-1 -\\theta^0_1*(z-1)\\right)p(z|y) = (1-\\theta^0_1) * \\sum_{z=1}^n(z-1)p(z|y),\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    E_4 = \\E(n - z + 1 - \\sum_{i=z}^{n}y_i|\\theta^0) = \\sum_{z=1}^n\\left(n - z + 1 - (n-z+1) * \\theta^0_2 \\right)p(z|y) = (1-\\theta^0_2) * \\sum_{z=1}^n(n-z+1)p(z|y) = (1-\\theta^0_2) * \\left(n - \\sum_{z=1}^n(z-1)p(z|y)\\right).\n",
    "\\end{equation}\n",
    "Therefore to compute the exquations above, we need to compute $\\sum_{z=1}^n(z-1)p(z|y)$ which we can calculate inside the first for loop of the previous algorithm. The complexity of the whole algorithm will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\t{\\theta}$\n",
    "$\\def\\z{j}$\n",
    "$\\def\\tt{{\\theta}^0}$\n",
    "\n",
    "Question 6\n",
    "-------\n",
    "To estimate the parameter $\\theta$, the Expecation-Maximization (EM) algorithm. We find\n",
    "\\begin{align}\n",
    "\tQ(\\theta, \\theta^0 ) &= \\E\\left[\\log(p(y,z \\mid \\theta)\\mid y,\\theta^0)\\right] \\\\\n",
    "    &= \\sum_{j = 1}^n \\log\\left(p(y,z = j \\mid \\theta)\\right) * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log\\left(p(z = j \\mid \\theta\\right)\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log{\\frac{1}{n}}\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &= \\sum_{j = 2}^n \\left[\\left(\\log{\\theta_1} * \\sum_{i=1}^{z-1}y_i + \\log{(1 - \\theta_1)} * (z - 1 + \\sum_{i=1}^{z-1}y_i) + \\log{\\theta_2} * \\sum_{i=z}^{n}y_i + \\log{(1 - \\theta_2)} * (n - z + 1 + \\sum_{i=z}^{n}y_i)\\right) + \\log{\\frac{1}{n}}\\right] * p(z = j \\mid y,\\theta^0) \\\\\n",
    "    &+ \\left(\\log{\\theta_2} * \\sum_{i=1}^n y_i + \\log{(1-\\theta_2)} * (n - \\sum_{i=1}^n y_i) + \\log{\\frac{1}{n}}\\right) * p(z = 1 \\mid y,\\theta^0),\n",
    "\\end{align}\n",
    "where we deduce (**remove?**) $p(z = j \\mid y,\\theta^0) = \\cfrac{ p(y \\mid z = j, \\theta^0) }{\\sum_{k=1}^n p(y \\mid z = k, \\theta^0)}$ using the Bayes formula and the fact that $p(y \\mid \\theta^0) = \\sum_{k=1}^n p(y \\mid z = k, \\theta^0) p (z = k \\mid \\theta^0)$.\n",
    "\n",
    "To maximize the provided function and make one step of the EM algorithm, we find derivatives of $Q(\\theta, \\theta^0)$ with respect to $\\theta = (\\theta_1,\\theta_2)$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_1} &= \\sum_{\\z = 2}^n  \\left( \\cfrac{1}{\\t_1} \\sum_{i=1}^{\\z-1}y_i - \\cfrac{1}{1-\\t_1}(\\z -1 -\\sum_{i=1}^{\\z-1}y_i )\\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_1(1-\\t_1)} \\sum_{\\z = 2}^n  \\left( (1-\\t_1) \\sum_{i=1}^{\\z-1}y_i - \\t_1 (\\z -1 -\\sum_{i=1}^{\\z-1}y_i) \\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "   &= \\frac{1}{\\t_1(1-\\t_1)} \\left(\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt) \\sum_{i=1}^{\\z-1}y_i - \\t_1\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt)(\\z -1)\\right),\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_2} &= \\sum_{\\z = 1}^n \\left( \\cfrac{1}{\\t_2} \\sum_{i=\\z}^{n}y_i - \\cfrac{1}{1-\\t_2}(n - \\z +1 -\\sum_{i=\\z}^{n}y_i) \\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_2(1-\\t_2)} \\sum_{\\z = 1}^n  \\left( (1-\\t_2) \\sum_{i=\\z}^{n}y_i - \\t_2 (n - \\z +1 -\\sum_{i=\\z}^{n}y_i )\\right) * p(z = \\z \\mid y,\\tt) \\\\\n",
    "    &= \\frac{1}{\\t_2(1-\\t_2)} \\left( \\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt) \\sum_{i=\\z}^{n}y_i - \\t_2\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt)(n- \\z + 1) \\right).\\\\\n",
    "\\end{align}\n",
    "\n",
    "If we solve $\\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_1} = 0$ and $\\cfrac{\\delta Q(\\t, \\tt )}{\\delta \\t_2} = 0$, we obtain the formulas for the EM algorithm maximising $Q(\\t,\\tt)$:\n",
    "\\begin{equation}\n",
    "\\t_1 = \\cfrac{\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt) \\sum_{i=1}^{\\z-1}y_i}{\\sum_{\\z = 2}^n p(z = \\z \\mid y,\\tt)(\\z -1)},\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\t_2 = \\cfrac{\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt) \\sum_{i=\\z}^{n}y_i}{\\sum_{\\z = 1}^n p(z = \\z \\mid y,\\tt)(n -\\z +1)}.\n",
    "\\end{equation}\n",
    "This gives us the following algorithm where $\\epsilon$ is some predefined small constant:\n",
    "<img src=\"img/p1q6b.png\" alt=\"Algorithm\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7\n",
    "-------\n",
    "To evalute the derived algorithm, we generate sequence of length 321 with some randomly chosen values of $\\theta$:\n",
    "```python\n",
    "generation with theta = (0.910192,0.234319), z = 52\n",
    "```\n",
    "Then we run the EM algorithm to estimate $\\theta$ and $z$. We use $\\epsilon = 0.0001$ and $L_2$ norm as a convergence criteria:\n",
    "```python\n",
    "iter 0, cur theta = (0.527564,0.376617)\n",
    "iter 1, cur theta = (0.831720,0.186754)\n",
    "iter 2, cur theta = (0.880847,0.185413)\n",
    "iter 3, cur theta = (0.881614,0.185544)\n",
    "estimated z = 52, theta = (0.881620,0.185547)\n",
    "```\n",
    "<img src=\"img/p1q7.png\" alt=\"Histogram\" style=\"width: 600px;\"/>\n",
    "As we can see, there is only one pick in the histogram which has value about 0.7. This means that our algorithm does good job and does not hesitate between neighboring values of $z$. The algorithm needed only 4 iterations to converge which takes a few fractions of second. However, values of the estimated $\\theta$ are not exactly true, however very close. This is the consequence of the finite size of the sequence and presenting noise. The bigger the sequence is, the better the law of big numbers works (and therefore the closer our estimation of the mathematical expectation) so the better our algorithm will work.\n",
    "\n",
    "We ran the algorithm a few more times and observed exactly the same behaviour. We do not provide histograms here as they look exactly the same as the previous one. A few runs with random $\\theta$ initializtion are provided below:\n",
    "```python\n",
    "iter 0, cur theta = (0.447207,0.625708)\n",
    "iter 1, cur theta = (0.298836,0.000157)\n",
    "iter 2, cur theta = (0.315317,0.014550)\n",
    "iter 3, cur theta = (0.319093,0.042511)\n",
    "iter 4, cur theta = (0.398426,0.102089)\n",
    "iter 5, cur theta = (0.522450,0.137815)\n",
    "iter 6, cur theta = (0.706709,0.174965)\n",
    "iter 7, cur theta = (0.873219,0.184871)\n",
    "iter 8, cur theta = (0.881541,0.185521)\n",
    "estimated z = 52, thetas = (0.881620,0.185546)\n",
    "```\n",
    "```python\n",
    "iter 0, cur theta = (0.734940,0.254173)\n",
    "iter 1, cur theta = (0.876291,0.185521)\n",
    "iter 2, cur theta = (0.881570,0.185532)\n",
    "estimated z = 52, thetas = (0.881620,0.185547)\n",
    "```\n",
    "This shows that the data is not too difficult to the method as during the whole history of observation it converged to the global maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8\n",
    "-------\n",
    "We have chosen the uniform distribution for $z$ as we have no prior knowledge about which $z$ is more probable than another. This approach increases the entropy of the distribution and is widely used in practice. However, if we have some knowledge about distribution of $z$, we would have to consider it while deducing the formulas. For example, the formula for $Q(\\theta, \\theta^0 )$ would change to the following:\n",
    "\\begin{align}\n",
    "    Q(\\theta, \\theta^0 ) &= \\E\\left[\\log(p(y,z \\mid \\theta)\\mid y,\\theta^0)\\right] \\\\\n",
    "    &= \\sum_{j = 1}^n \\log\\left(p(y,z = j \\mid \\theta)\\right) * p(z = j \\mid y,\\theta^0)) \\\\\n",
    "    &= \\sum_{j = 1}^n \\left[\\log\\left(p(y \\mid z = j,\\theta)\\right) + \\log\\left(p(z = j \\mid \\theta\\right)\\right] * p(z = j \\mid y,\\theta^0)),\n",
    "\\end{align}\n",
    "where $p(z = j \\mid y,\\theta^0) = \\cfrac{ p(y \\mid z = j, \\theta^0) p(z=j \\mid \\theta^0)}{\\sum_{k=1}^n p(y \\mid z = k, \\theta^0) p(z=k \\mid \\theta^0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c356bf9e135e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c356bf9e135e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <http://someurl>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".<http://someurl>\n",
    "\n",
    "<somebbob@example.com>\n",
    "\n",
    "$\\theta^* = \\argmax_\\theta logp(y|z,\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
